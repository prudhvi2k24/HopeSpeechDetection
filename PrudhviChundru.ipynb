{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6oD-UPJIHJ8",
        "outputId": "5ffc9148-a0a5-4618-ff03-0c437ce9f979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How to train the model.\n",
            "1. Binary\n",
            " 2. Multiclass\n",
            " Your Response:2\n",
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
            "Best Parameters for multiclass: {'classifier__max_depth': None, 'classifier__n_estimators': 200}\n",
            "Best Score for multiclass: 0.8855445337241866\n",
            "Accuracy (multiclass): 0.7000955109837631\n",
            "Classification Report (multiclass):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.88      0.82       482\n",
            "           1       0.52      0.61      0.56       232\n",
            "           2       0.32      0.08      0.13       102\n",
            "           3       0.51      0.35      0.42        91\n",
            "           4       0.91      0.90      0.91       140\n",
            "\n",
            "    accuracy                           0.70      1047\n",
            "   macro avg       0.61      0.56      0.57      1047\n",
            "weighted avg       0.67      0.70      0.67      1047\n",
            "\n",
            "Predictions saved to en_test_without_labels.csv\n",
            "Successfully Executed\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import unicodedata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from sklearn.impute import SimpleImputer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Load required NLTK resources\n",
        "for resource in ['vader_lexicon', 'punkt', 'stopwords']:\n",
        "    try:\n",
        "        nltk.data.find(f'corpora/{resource}')\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and tokenizes text.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http\\S+|www\\S+|@\\S+', '', text)  # Remove URLs and mentions\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only alpha characters and spaces\n",
        "    text = ''.join(c for c in unicodedata.normalize('NFKD', text) if unicodedata.category(c) != 'Mn')  # Remove diacritics\n",
        "\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "def process_data():\n",
        "    \"\"\"Loads and processes the dataset.\"\"\"\n",
        "    file_path = r\"/en_train.csv\"\n",
        "    try:\n",
        "        df = load_data(file_path)\n",
        "        df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "        if 'binary' in df.columns:\n",
        "            df['binary'] = df['binary'].map({'Hope': 1, 'Not Hope': 0}).fillna(-1).astype(int)\n",
        "            df = df[df['binary'] != -1]  # Remove invalid labels\n",
        "\n",
        "        if 'multiclass' in df.columns:\n",
        "            df['multiclass'] = df['multiclass'].map({\n",
        "                'Not Hope': 0, 'Generalized Hope': 1, 'Realistic Hope': 2, 'Unrealistic Hope': 3, 'Sarcasm': 4\n",
        "            }).fillna(-1).astype(int)\n",
        "            df = df[df['multiclass'] != -1]\n",
        "\n",
        "        df.dropna(subset=['text', 'binary', 'multiclass'], inplace=True)\n",
        "        df['text'] = df['text'].astype(str)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "def extract_sentiment_features(text):\n",
        "    \"\"\"Extracts sentiment polarity features.\"\"\"\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return [scores['compound'], scores['pos'], scores['neg'], scores['neu']]\n",
        "\n",
        "def train_data(df, task='binary'):\n",
        "    \"\"\"Trains a model using TF-IDF and RandomForest for either binary or multi-class classification.\"\"\"\n",
        "    # Define the feature (X) and target (Y) columns\n",
        "    X = df['text']\n",
        "\n",
        "    # Select target based on the task\n",
        "    if task == 'binary':\n",
        "        Y = df['binary']\n",
        "    elif task == 'multiclass':\n",
        "        Y = df['multiclass']\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task. Choose either 'binary' or 'multiclass'.\")\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Apply TF-IDF to the text data\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    # Extract sentiment features\n",
        "    train_sentiment = pd.DataFrame(X_train.apply(extract_sentiment_features).tolist(), index=X_train.index)\n",
        "    test_sentiment = pd.DataFrame(X_test.apply(extract_sentiment_features).tolist(), index=X_test.index)\n",
        "\n",
        "    # Combine TF-IDF and sentiment features\n",
        "    X_train_combined = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), train_sentiment.reset_index(drop=True)], axis=1)\n",
        "    X_test_combined = pd.concat([pd.DataFrame(X_test_tfidf.toarray()), test_sentiment.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Handle class imbalance using SMOTENC\n",
        "    categorical_features = [X_train_combined.shape[1] - 4]  # Last 4 columns are sentiment features\n",
        "    smote = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)\n",
        "\n",
        "    # Fit SimpleImputer to handle missing values\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train_imputed = imputer.fit_transform(X_train_resampled)\n",
        "    X_test_imputed = imputer.transform(X_test_combined)\n",
        "\n",
        "    # Train Random Forest Model\n",
        "    pipeline = Pipeline([('classifier', RandomForestClassifier(random_state=42))])\n",
        "    param_grid = {'classifier__n_estimators': [50, 100, 200], 'classifier__max_depth': [None, 10, 20]}\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', verbose=1)\n",
        "    grid_search.fit(X_train_imputed, y_train_resampled)\n",
        "\n",
        "    print(f\"Best Parameters for {task}:\", grid_search.best_params_)\n",
        "    print(f\"Best Score for {task}:\", grid_search.best_score_)\n",
        "\n",
        "    # Evaluate Model\n",
        "    y_pred = grid_search.best_estimator_.predict(X_test_imputed)\n",
        "    print(f\"Accuracy ({task}): {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"Classification Report ({task}):\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "    return vectorizer, imputer, grid_search.best_estimator_\n",
        "\n",
        "\n",
        "def predict_label(model, text, vectorizer, imputer):\n",
        "    \"\"\"Predicts the label of a single text entry.\"\"\"\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    sentiment_features = extract_sentiment_features(text)\n",
        "\n",
        "    text_tfidf = vectorizer.transform([preprocessed_text])\n",
        "    text_combined = pd.concat([pd.DataFrame(text_tfidf.toarray()), pd.DataFrame([sentiment_features])], axis=1)\n",
        "    text_imputed = imputer.transform(text_combined)\n",
        "    return model.predict(text_imputed)[0]\n",
        "\n",
        "def predict_from_excel(model, excel_file, output_column, vectorizer, imputer):\n",
        "    \"\"\"Loads an Excel file and applies predictions.\"\"\"\n",
        "    df = pd.read_csv(excel_file)\n",
        "    df.columns = df.columns.str.lower()\n",
        "    if 'text' not in df.columns:\n",
        "        raise ValueError(\"Excel file must contain a 'text' column.\")\n",
        "\n",
        "    df['Processed_text'] = df['text'].apply(preprocess_text)\n",
        "    df[output_column] = df['Processed_text'].apply(lambda x: predict_label(model, x, vectorizer, imputer))\n",
        "    df.rename(columns={'text': 'Text'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = process_data()\n",
        "    method = int(input(\"How to train the model.\\n1. Binary\\n 2. Multiclass\\n Your Response:\"))\n",
        "    if(method == 1):\n",
        "        vec, imp, model = train_data(df.copy(),\"binary\")\n",
        "    elif(method == 2):\n",
        "        vec, imp, model = train_data(df.copy(),\"multiclass\")\n",
        "    excel_file = \"/en_test_without_labels.csv\"\n",
        "    df1 = predict_from_excel(model, \"/en_test_without_labels.csv\", \"Tag\", vec, imp)\n",
        "    if(method == 1):\n",
        "        df1['Tag'] = df1['Tag'].map({1 : 'Hope', 0 : 'Not Hope' }).fillna(-1).astype(str)\n",
        "    elif(method == 2):\n",
        "        df1['Tag'] = df1['Tag'].map({0 : 'Not Hope', 1 : 'Generalized Hope' , 2 : 'Realistic Hope', 3: 'Unrealistic Hope' , 4 : 'Sarcasm' }).fillna(-1).astype(str)\n",
        "    del df1['Processed_text']\n",
        "    df1.to_csv(excel_file, index=False)\n",
        "    print(f\"Predictions saved to en_test_without_labels.csv\")\n",
        "    print(\"Successfully Executed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhI34YtLtx3s"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}